---
title: "PESI analysis with brms"
author: "I. S. Plank"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r settings, include=FALSE}

knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.align = 'center', fig.width = 9)
ls.packages = c("knitr",            # kable
                "ggplot2",          # plots
                "brms",             # Bayesian lmms
                "designr",          # simLMM
                "bridgesampling",   # bridge_sampler
                "tidyverse",        # tibble stuff
                "ggpubr",           # ggarrange
                "ggrain",           # geom_rain
                "bayesplot",        # plots for posterior predictive checks
                "SBC",              # plots for checking computational faithfulness
                "rstatix",          # anova
                "readODS",          # reading in ODS spreadsheets
                "vtable",           # creating a summary table
                "BayesFactor", 
                "bayestestR"
                )

lapply(ls.packages, library, character.only=TRUE)

# set cores
options(mc.cores = parallel::detectCores())

# graph settings 
c_light = "#a9afb2"; c_light_highlight = "#8ea5b2"; c_mid = "#6b98b2" 
c_mid_highlight = "#3585b2"; c_dark = "#0072b2"; c_dark_highlight = "#0058b2" 
c_green = "#009E73"
sz = 1

# custom colour palette
custom.col = c(c_dark_highlight, c_green, "#CC79A7", "#D55E00")

# settings for the SBC package
use_cmdstanr = getOption("SBC.vignettes_cmdstanr", TRUE) # Set to false to use rst
options(brms.backend = "cmdstanr")
cache_dir = "./_brms_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

# use parallel processing
library(future)
plan(multisession)

# calculate visual angle: px.x should be a vector of c(width, height) in pixel
vis_ang = function(px.x) {
  
  # infos on the setup
  mm.w = 533    # monitor width in mm
  mm.h = 300    # monitor height in mm
  px.w = 1920   # monitor resolution: width
  px.h = 1080   # monitor resolution: height
  dist = 570    # viewing distance
  
  # convert width from screen pixel size to mm and take half
  mm.x    = (px.x[1] / (px.w/mm.w))/2
  
  # convert height from screen pixel size to mm and take half
  mm.x[2] = (px.x[2] / (px.h/mm.h))/2
  
  # calculate angles
  rad.alpha    = atan(mm.x[1]/dist)
  rad.alpha[2] = atan(mm.x[2]/dist)
  
  # convert to degrees and double again
  deg.alpha    = (rad.alpha[1] / (pi/180)) * 2
  deg.alpha[2] = (rad.alpha[2] / (pi/180)) * 2
  
  return(deg.alpha)
  
}

```

<style type="text/css">
.main-container {
  max-width: 1100px;
  margin-left: auto;
  margin-right: auto;
}
</style>

# Introduction

The PESI project focuses on "Perception of nonverbal social interaction in people with and without autism". Specifically, autistic and non-autistic comparison observers watched 64 silent videos of dyadic interactions between two non-autistic or one autistic and one non-autistic adult. The visual angle of the interaction was approximately `r round(vis_ang(c(950, 650))[1],2)` degrees wide and `r round(vis_ang(c(950, 650))[2],2)` degrees high, depending on the exact frame. We created circular areas of interest (AOIs) for dwell times on the heads and the hands. Their visual angle were `r round(vis_ang(c(210, 210))[1],2)` and `r round(vis_ang(c(140, 140))[1],2)` degrees, respectively. 

All observers were adults. These videos were 10 seconds of longer interactions and either portrayed moments of high or low interpersonal synchrony (IPS) of movement. The videos only showed outlines of the dyad to direct the focus on the movement. During the video presentation, observers' gaze is tracked and after each video they are asked to rate how comfortable they imagine this interaction to be. 

We preregistered the following hypotheses:

\begin{enumerate}
  \item Ratings
  \begin{enumerate}
    \item Dyad type: Social interactions of no-diagnosis non-autistic dyads are rated more positively than mixed-diagnosis dyads consisting of one autistic and one non-autistic interaction partner. 
    \item Synchrony: Social interactions with high interpersonal synchrony of motion energy are rated more positively than social interactions with low interpersonal synchrony of motion energy. 
    \item Diagnostic status: Ratings of social interactions differ between autistic and non-autistic comparison observers. 
    \item Synchrony x dyad type: The effect of interpersonal motion synchrony on ratings is decreased for mixed-diagnosis dyads compared to no-diagnosis dyads. 
    \item Dyad type x diagnostic status: The effect of dyad type on ratings is decreased in autistic compared to comparison observers.
  \end{enumerate}
  \item Fixation duration in areas of interest (AOI)
  \begin{enumerate}
    \item Dyad type x AOI: Fixation durations for each area of interest differ between clips of no-diagnosis and mixed-diagnosis dyads.
    \item Synchrony x AOI: Fixation durations for each area of interest differ between high and low synchrony clips.
    \item Diagnostic status x AOI: Fixation durations for each area of interest differ between autistic and comparison observers. 
  \end{enumerate}
\end{enumerate}

## Some general settings

```{r set}

# number of simulations
nsim = 500

# set number of iterations and warmup for models
iter = 4500
warm = 1500

# set the seed
set.seed(2468)

```

## Package versions

```{r lib_versions, echo=F}

print(R.Version()$version.string)

for (package in ls.packages) {
  print(sprintf("%s version %s", package, packageVersion(package)))
}

```

## General info

We planned to determine the group-level effect subjects following Barr (2013). For each model, experiment specific priors were set based on previous literature or the task (see comments in the code).

We perform prior predictive checks as proposed in Schad, Betancourt and Vasishth (2020) using the SBC package. To do so, we create `r print(nsim)` simulated datasets where parameters are simulated from the priors. These parameters are used to create one fake dataset. Both the true underlying parameters and the simulated values are saved.

Then, we create graphs showing the prior predictive distribution of the simulated discrimination threshold to check whether our priors fit our general expectations about the data. Next, we perform checks of computational faithfulness and model sensitivity as proposed by Schad, Betancourt and Vasishth (2020) and implemented in the SBC package. We create models for each of the simulated datasets. Last, we calculate performance metrics for each of these models, focusing on the population-level parameters.

## Preparation and group comparisons 

First, we load the data and combine it with demographic information including the diagnostic status of the subjects. Then, all predictors are set to sum contrasts. We have a look at the demographics describing our two diagnostic groups: autistic adults and adults without any neurological and psychiatric diagnoses. 

Since this is sensitive data, we load the anonymised version of the processed data at this point but also leave the code we used to create it. 

```{r prep_data}

# check if the data file exists, if yes load it:
if (!file.exists("PESI_data.RData")) {
  
  # set file paths
  fl.path = '/media/emba/emba-2/PESI'
  dt.path = paste(fl.path, 'BVET', sep = "/")
  
  # read in list of participants
  df.inc = read_ods(file.path(fl.path, "PESI_inc.ods"), range = "A1:K100") %>%
    select(subID, include_BV, include_ET)
  
  # create an anonymisation key
  df.inc = df.inc %>%
    mutate(
      PID   = subID,
      subID = as.numeric(as.factor(subID))
    )
  df.recode = df.inc %>% select(PID, subID) %>% distinct()
  recode = as.character(df.recode$subID)
  names(recode) = df.recode$PID
  
  # filter out pilots and cancelled testings
  df.inc = df.inc %>%
    filter(include_BV == 1)
  
  # load the relevant data in long format
  df.beh = list.files(path = dt.path, pattern = "PESI-BV", full.names = T) %>%
    map_df(~read_delim(., show_col_types = F, delim = ",",
                       col_types = "cddcccddddd")) %>%
    select(-dyad, -sync) %>%
    filter(subID %in% df.inc$PID) %>%
    group_by(subID) %>%
    mutate(
      no_trials = n()
    ) %>%
    # filter out participants where not the full task was recorded > no participants
    filter(no_trials == 64)
  
  # load demographic information to get diagnostic status 
  df.sub = read_csv(file.path(dt.path, "PESI_centraXX.csv"), show_col_types = F) %>%
    mutate(
      diagnosis = recode(diagnosis, "CTR" = "COMP")
    ) 
  
  # load the stimulus description file
  df.stm = read_csv(paste(fl.path, "PESI_videosel-full_230404.csv", 
                          sep = "/"), show_col_types = F) %>%
    mutate(
      video = sprintf("PESI_%s_%08d", substr(dyad,1,3), frame_sta),
      dyad.type = case_match(context, 
                             "homogeneous" ~ "non-autistic",
                             "heterogeneous" ~ "mixed")
    ) %>%
    select(video, dyad, sync, dyad.type, mot, peak, s1peak, s2peak)
  
  # merge together
  df = merge(df.beh, df.stm, all.x = T, by = "video") %>%
    mutate(
      # only use trials with confirmed rating and correct video duration
      use = if_else(confirmed == 1 & abs(dur-10) < 0.1, 1,0), 
      rating.confirmed = if_else(use == 1, rating, NA)
           ) %>%
    arrange(subID, trl)

  # merge with group information 
  df = merge(df.sub %>% select(subID, diagnosis), df, all.y = T) %>%
    mutate_if(is.character, as.factor)
  
  # check how many participants per group need to be excluded
  df.exc = df %>%
    group_by(subID, diagnosis) %>%
    summarise(
      total = sum(use)/64
    ) %>%
    filter(total <= 2/3) %>%
    group_by(diagnosis) %>%
    summarise(
      n = n()
    )
  
  # exclude participants with more than 33% of trials missing
  df = df %>%
    group_by(subID) %>%
    mutate(
      total = sum(use)/64
    ) %>%
    filter(total > 2/3)
  
  # get a list of the subIDs
  subIDs = unique(df$subID)
  
  # anonymise the data
  df$subID = str_replace_all(df$subID, recode)
  
  # load preprocessed eye tracking data and rename variables
  df.fix = readRDS(file.path(dt.path, "PESI-ET_fix.rds")) %>%
    rename(
      "trl" = "on_trialNo", "video" = "on_trialVid"
    )
  df.sac.agg = readRDS(file.path(dt.path, "PESI-ET_sac.rds")) %>%
    rename(
      "trl" = "on_trialNo", "video" = "on_trialVid"
    )
  
  # add information on the videos to the eye tracking data
  df.fix = merge(df.fix, df.stm, all.x = T) %>%
    # add diagnostic group
    merge(., df.sub %>% select(subID, diagnosis), all.x = T) %>%
    mutate(across(where(is.character), as.factor))
  df.sac.agg = merge(df.sac.agg, df.stm, all.x = T) %>%
    # add diagnostic group
    merge(., df.sub %>% select(subID, diagnosis), all.x = T) %>%
    mutate(across(where(is.character), as.factor))
  
  # add subjects to the subIDs
  subIDs = unique(c(subIDs, df.fix$subID))
  subIDs = unique(c(subIDs, df.sac.agg$subID))
  
  # get numbers of participants included in ET per group and gender identity
  df.incET = merge(df.fix %>% 
                     select(subID, diagnosis) %>% 
                     distinct(),
                   df.sub %>% select(subID, gender)
                   ) %>%
    group_by(diagnosis, gender) %>%
    summarise(
      n = n()
    )
  
  # anonymise ET data in the same way as the behavioural data
  df.fix$subID     = str_replace_all(df.fix$subID, recode)
  df.sac.agg$subID = str_replace_all(df.sac.agg$subID, recode)
  
  # remove participants not included in the behavioural or the eye tracking
  # analysis from the subjects data frame
  df.sub = df.sub %>% 
    filter(subID %in% subIDs)
  
  # get an overview over the diagnosis of the autistic group
  df.asd = df.sub %>% 
    filter(diagnosis == "ASD") %>%
    group_by(ASD.icd10) %>%
    count()
  
  # print gender frequencies and compare them across groups
  tb.gen = xtabs(~ gender + diagnosis, data = df.sub)
  ct.full = contingencyTableBF(tb.gen, sampleType = "indepMulti", fixedMargin = "cols")
  
  # check which outcomes of interest are normally distributed
  df.sht = df.sub %>% 
    group_by(diagnosis) %>%
    shapiro_test(age, CFT_iq, BDI_total, STAITT_total, RAADS_total, ISH_total, 
                 UI_total, sw_gz, sw_kl, thre, steep) %>% 
    arrange(variable) %>%
    mutate(
      sig = if_else(p < 0.05, "*", "")
    )
  
  # some of the measures are not normally distributed; 
  # therefore, we compute ranks for these outcomes
  df.sub = df.sub %>% 
    mutate(
      rBDI   = rank(BDI_total),
      rISH   = rank(ISH_total),
      rRAADS = rank(RAADS_total),
      rUI    = rank(UI_total),
      rage   = rank(age),
      rsw_kl = rank(sw_kl),
      diagnosis = as.factor(diagnosis)
    )
  
  # now we can compute our ttests
  ostt.age    = ttestBF(formula = rage         ~ diagnosis, data = df.sub)
  ostt.iq     = ttestBF(formula = CFT_iq       ~ diagnosis, data = df.sub)
  ostt.kl     = ttestBF(formula = sw_gz        ~ diagnosis, data = df.sub)
  ostt.gz     = ttestBF(formula = rsw_kl       ~ diagnosis, data = df.sub)
  ostt.STAITT = ttestBF(formula = STAITT_total ~ diagnosis, data = df.sub)
  ostt.BDI    = ttestBF(formula = rBDI         ~ diagnosis, data = df.sub)
  ostt.ISH    = ttestBF(formula = rISH         ~ diagnosis, data = df.sub)
  ostt.RAADS  = ttestBF(formula = rRAADS       ~ diagnosis, data = df.sub)
  ostt.UI     = ttestBF(formula = rUI          ~ diagnosis, data = df.sub)
  
  # ...and put everything in a new dataframe for printing
  measurement  = "Age"
  ASD      = sprintf("%.2f (±%.2f)", 
                     mean(df.sub[df.sub$diagnosis == "ASD",]$age), 
                     sd(df.sub[df.sub$diagnosis == "ASD",]$age)/
                       sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",])))
  COMP      = sprintf("%.2f (±%.2f)", 
                      mean(df.sub[df.sub$diagnosis == "COMP",]$age), 
                      sd(df.sub[df.sub$diagnosis == "COMP",]$age)/
                        sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",])))
  logBF10 = sprintf("%.3f", ostt.age@bayesFactor[["bf"]])
  df.table = data.frame(measurement, ASD, COMP, logBF10)
  df.table = rbind(df.table,
                   c(
                     "BDI",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$BDI_total), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$BDI_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$BDI_total), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$BDI_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.BDI@bayesFactor[["bf"]])
                   ),
                   c(
                     "Gender (diverse/agender/non-binary - female - male)",
                     sprintf("%d - %d - %d",
                             nrow(df.sub[df.sub$diagnosis == "ASD" & 
                                           df.sub$gender == "dan",]), 
                             nrow(df.sub[df.sub$diagnosis == "ASD" & 
                                           df.sub$gender == "fem",]), 
                             nrow(df.sub[df.sub$diagnosis == "ASD" & 
                                           df.sub$gender == "mal",])),
                     sprintf("%d - %d - %d", 
                             nrow(df.sub[df.sub$diagnosis == "COMP" & 
                                           df.sub$gender == "dan",]), 
                             nrow(df.sub[df.sub$diagnosis == "COMP" & 
                                           df.sub$gender == "fem",]), 
                             nrow(df.sub[df.sub$diagnosis == "COMP" & 
                                           df.sub$gender == "mal",])),
                     sprintf("%.3f", ct.full@bayesFactor[["bf"]])
                   ),
                   c(
                     "IQ",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$CFT_iq), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$CFT_iq)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$CFT_iq), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$CFT_iq)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.iq@bayesFactor[["bf"]])
                   ),
                   c(
                     "RAADS",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$RAADS_total), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$RAADS_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$RAADS_total), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$RAADS_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.RAADS@bayesFactor[["bf"]])
                   ),
                   c(
                     "D2 - concentration performance",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$sw_kl), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$sw_kl)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$sw_kl), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$sw_kl)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.kl@bayesFactor[["bf"]])
                   ),
                   c(
                     "D2 - speed",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$sw_gz), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$sw_gz)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$sw_gz), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$sw_gz)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.gz@bayesFactor[["bf"]])
                   ),
                   c(
                     "STAI-trait",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$STAITT_total), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$STAITT_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$STAITT_total), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$STAITT_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.STAITT@bayesFactor[["bf"]])
                   ),
                   c(
                     "Ishihara",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$ISH_total), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$ISH_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$ISH_total), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$ISH_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.ISH@bayesFactor[["bf"]])
                   ),
                   c(
                     "UI",
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "ASD",]$UI_total), 
                             sd(df.sub[df.sub$diagnosis == "ASD",]$UI_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "ASD",]))),
                     sprintf("%.2f (±%.2f)", 
                             mean(df.sub[df.sub$diagnosis == "COMP",]$UI_total), 
                             sd(df.sub[df.sub$diagnosis == "COMP",]$UI_total)/
                               sqrt(nrow(df.sub[df.sub$diagnosis == "COMP",]))),
                     sprintf("%.3f", ostt.UI@bayesFactor[["bf"]])
                   )
                ) %>% arrange(measurement)
  
  # save it all
  df = df %>% select(subID, diagnosis, dyad, video, run, trl, sync, rating, 
                     dyad.type, mot, peak, rating.confirmed, s1peak, s2peak)
  save(df, df.table, df.sht, ct.full, df.exc, df.incET, df.fix, df.sac.agg, df.asd, 
       file = "PESI_data.RData")
  
} else {
  
  load("PESI_data.RData")
  
}

# print the group of included participants
kable(df %>% select(subID, diagnosis) %>% distinct() %>% group_by(diagnosis) %>% count())

# print the group of excluded participants
kable(df.exc)
rm(df.exc)

# print number of included participants in eye tracking
kable(df.incET)
rm(df.incET)

# print the icd 10 code counts
kable(df.asd)
rm(df.asd)

# print the outcome of the shapiro tests
kable(df.sht)
rm(df.sht)

# print the outcome of the contingency table
ct.full@bayesFactor

# aggregate the data due to large differences between videos
df.agg = df %>% 
  group_by(subID, diagnosis, dyad, sync, dyad.type) %>%
  summarise(
    rating.confirmed = mean(rating.confirmed, na.rm = T)
  ) %>% ungroup() %>%
  mutate_if(is.character, as.factor)
df.fix.agg = df.fix %>% 
  group_by(subID, diagnosis, dyad, sync, dyad.type, AOI) %>%
  summarise(
    fix.dur   = median(fix.dur, na.rm = T),
    fix.total = median(fix.total),
    fix.prop  = fix.dur*100/fix.total
  ) %>% ungroup() %>%
  mutate_if(is.character, as.factor)

# set and print the contrasts
contrasts(df.agg$sync) = contr.sum(2)
contrasts(df.agg$sync)
contrasts(df.agg$dyad.type) = contr.sum(2)
contrasts(df.agg$dyad.type)
contrasts(df.agg$diagnosis) = contr.sum(2)
contrasts(df.agg$diagnosis)
contrasts(df.fix$sync) = contr.sum(2)
contrasts(df.fix$sync)
contrasts(df.fix$dyad.type) = contr.sum(2)
contrasts(df.fix$dyad.type)
contrasts(df.fix$diagnosis) = contr.sum(2)
contrasts(df.fix$diagnosis)
contrasts(df.fix.agg$sync) = contr.sum(2)
contrasts(df.fix.agg$sync)
contrasts(df.fix.agg$dyad.type) = contr.sum(2)
contrasts(df.fix.agg$dyad.type)
contrasts(df.fix.agg$diagnosis) = contr.sum(2)
contrasts(df.fix.agg$diagnosis)
contrasts(df.fix.agg$AOI) = contr.sum(3)[c(3,2,1),]
contrasts(df.fix.agg$AOI)
contrasts(df.sac.agg$sync) = contr.sum(2)
contrasts(df.sac.agg$sync)
contrasts(df.sac.agg$dyad.type) = contr.sum(2)
contrasts(df.sac.agg$dyad.type)
contrasts(df.sac.agg$diagnosis) = contr.sum(2)
contrasts(df.sac.agg$diagnosis)

# print final group comparisons for the paper
kable(df.table)

```

The two diagnostic groups are similar in age, concentration and speed (D2), colorblindness score (Ishihara), IQ and gender distribution as well as on a perceptual simultaneity task (PST). However, they seem to differ in their questionnaire scores measuring depressive symptoms (BDI), autism-like traits (RAADS), trait anxiety (STAI-trait) and intolerance of uncertainty (UI). 

# Hypothesis-guided analysis

## Aggregated ratings 

After each video, participants were asked to rate the interaction on a continuous scale for "How comfortable to you imagine this interaction to be?" (from 0 "not at all" to 100 "very"). 

### SBC

To achieve good posterior fit, we aggregated the rating data. First, we attempted to run a model including all main effects as slopes as well as the interaction of sync and dyad type for subjects. However, all models including random slopes had major divergence issues. Even the model only including the intercepts on the group-level had some divergent transitions. Nonetheless, we opted to include both the intercept for the dyads and the subjects, and look out for problems with the actual model. 

```{r sbc_setup, fig.height=24}

# set formula considering all combinations
code   = "PESI_int"
f.pesi = brms::bf(rating.confirmed ~ diagnosis * sync * dyad.type 
                  + (1 | subID) + (1 | dyad))

# set weakly informed priors
priors = c(
  prior(normal(50,  10), class = Intercept),
  prior(normal(0,   10), class = sigma),
  prior(normal(0,   10), class = sd),
  # differences due to dyad.type
  prior(normal(-5,   5), class = b, coef = dyad.type1), # mixed
  # differences due to synchrony
  prior(normal(5,    5), class = b, coef = sync1), # high
  # effect of synchrony decreased when mixed dyad
  prior(normal(-5,   5), class = b, coef = sync1:dyad.type1),
  # effect of dyad type decreased in autistic subjects
  prior(normal(-5,   5), class = b, coef = diagnosis1:dyad.type1),
  # no specific expectations for diagnostic groups and other interactions
  prior(normal(0,    5), class = b)
)

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # set the seed
  set.seed(2469)
  # create the data
  gen  = SBC_generator_brms(f.pesi, data = df.agg, prior = priors, 
                            thin = 50, warmup = 20000, refresh = 2000
  )
  dat  = generate_datasets(gen, nsim)
  saveRDS(dat, file = sprintf("%s/dat_%s.rds", cache_dir, code))
  
  # perform the SBC
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1, 
                                        warmup = warm, iter = iter,               
                                        inits = 0.1)
  res = compute_SBC(dat, bck,
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  # save the results dataframes
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  
}

```

We start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). We will have to watch out for this. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_checks1, fig.height=6}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.pesi)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# plot simulated data for prior predictive checks
dvmax = 100
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH > dvmax] = dvmax 
dvfakematH[dvfakematH < 0] = 0 
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 100) 
binwidth = round(breaks[2] - breaks[1])
breaks = seq(0, max(dvfakematH), by = binwidth) 
histmat = matrix(NA, ncol = dim(dvfakematH)[2] + binwidth, nrow = length(breaks)-1) 
for (i in 1:dim(dvfakematH)[2]) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  xlim(0, max(dvfakematH)) +
  theme_bw()

```

The above figure shows the simulated data for the ratings. The data is centered around the middle of the rating scale. We had some outliers that were outside of the possible values (greater than 100 or smaller than 0), however, to keep our prior somewhat wide we chose to accept this in the simulated data. 

```{r sbc_checks2, fig.height=15}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(
  df.results.b, 
  prior_sd = setNames(c(10, # Intercept
                        rep(5, length(unique(df.results.b$variable))-1)), 
                      unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

Second, we check the ranks of the parameters. If the model is unbiased, these should be uniformly distributed (Schad, Betancourt and Vasishth, 2020). The sample empirical cumulative distribution function (ECDF) lies within the theoretical distribution (95%) and the rank histogram also shows ranks within the 95% expected range, although there are some small deviations. We judge this to be acceptable.

Third, we investigated the relationship between the simulated true parameters and the posterior estimates. Although there are individual values diverging from the expected pattern, most parameters were recovered successfully within an uncertainty interval of alpha = 0.05. 

Last, we explore the z-score and the posterior contraction of our population-level predictors. The z-score "determines the distance of the posterior mean from the true simulating parameter", while the posterior contraction "estimates how much prior uncertainty is reduced in the posterior estimation" (Schad, Betancourt and Vasisth, 2020). Both look acceptable. 

### Posterior predictive checks

As the next step, we fit the model and check whether there were any obvious issues with it. 

```{r postpc, fig.height = 9}

# fit the model
set.seed(2486)
m.pesi = brm(f.pesi,
            df.agg, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_PESI",
            save_pars = save_pars(all = TRUE)
            )

# in this model, there are no divergent samples
sum(subset(nuts_params(m.pesi), Parameter == "divergent__")$Value)

# check that rhats are below 1.01
sum(brms::rhat(m.pesi) >= 1.01, na.rm = T)

# and the chains have converged
post.draws = as_draws_df(m.pesi)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 2)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and no rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.pesi, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.pesi, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none") + xlim(0,100)

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.agg$rating.confirmed, post.pred, df.agg$diagnosis) + 
  theme_bw() + theme(legend.position = "none")
# ... sync level
p3 = ppc_stat_grouped(df.agg$rating.confirmed, post.pred, df.agg$sync) + 
  theme_bw() + theme(legend.position = "none")
# ... and dyad type
p4 = ppc_stat_grouped(df.agg$rating.confirmed, post.pred, df.agg$dyad.type) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3, p4,
          nrow = 2, ncol = 2, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks", 
                face = "bold", size = 14))

```

The predictions based on the model (light blue) capture the data (dark blue) very well. The predicted means for each group are firmly distributed around the real values. This further increased our trust in the model and we move on to interpret it.

### Model summary

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final, fig.height=6}

# print a summary
summary(m.pesi)

# plot the posterior distributions for the estimated effects
post.draws %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), 
               names_to = "coef", 
               values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ASD"),
    coef = str_replace_all(coef, "sync1", "high sync"),
    coef = str_replace_all(coef, "dyad.type1", "mixed"),
    coef_order = case_when(
      coef == "ASD" ~ 100,
      coef == "high sync" ~ 99,
      coef == "mixed" ~ 98,
      T ~ 100-nchar(coef)),
    coef = fct_reorder(coef, coef_order)
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>% 
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

```

### Inferences

```{r inf}

# H1.a Context: Social interactions of no-diagnosis non-autistic dyads are 
# rated more positively than mixed-diagnosis dyads consisting of one autistic 
# and one non-autistic interaction partner. 
h1.a = hypothesis(m.pesi, "dyad.type1 < 0")
h1.a

# H1.b Synchrony: Social interactions with high interpersonal synchrony of 
# motion energy are rated more positively than social interactions with low 
# interpersonal synchrony of motion energy. 
h1.b = hypothesis(m.pesi, "sync1 > 0")
h1.b 

# H1.c Diagnostic status: Ratings of social interactions differ between 
# autistic and comparison observers. 
h1.c = hypothesis(m.pesi, "diagnosis1 > 0", alpha = 0.025)
h1.c

# H1.d Synchrony x dyad type: The effect of interpersonal motion synchrony 
# on ratings is decreased for mixed-diagnosis dyads compared to no-diagnosis dyads. 
h1.d = hypothesis(m.pesi, "sync1:dyad.type1 < 0")
h1.d

# H1.e Dyad type x diagnostic status: The effect of dyad type on ratings is 
# decreased in autistic compared to comparison observers.
h1.e = hypothesis(m.pesi, "diagnosis1:dyad.type1 > 0")
h1.e

# Explore interaction between sync and dyad type in the COMP group only
e1 = hypothesis(m.pesi, 
                "0 < diagnosis1:sync1:dyad.type1 - sync1:dyad.type1", 
                alpha = 0.025)
e1

# extract predicted differences based on the model
df.new = df.agg %>% 
  select(diagnosis, dyad.type, sync) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, dyad.type, sync, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.pesi, summary = F, 
               newdata = df.new %>% select(diagnosis, dyad.type, sync), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

# compute specific differences
df.ms = df.ms %>%
  mutate(
    dyad.diff = 
      rowMeans(select(., matches(".*_non-autistic_.*")), na.rm = T) -
      rowMeans(select(., matches(".*_mixed_.*")), na.rm = T),
    sync.diff = 
      rowMeans(select(., ends_with("_high")), na.rm = T) -
      rowMeans(select(., ends_with("_low")), na.rm = T)
  )

# table for the cell values
st(df.ms, out = "kable")

```

The model revealed support for our hypotheses postulating decreased ratings for mixed dyads compared to non-autistic dyads (*estimate* = `r round(h1.a$hypothesis$Estimate,2)` [`r round(h1.a$hypothesis$CI.Lower,2)`, `r round(h1.a$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1.a$hypothesis$Post.Prob,3)`) and increased ratings for segments with high IPS as opposed to segments with low IPS (*estimate* = `r round(h1.b$hypothesis$Estimate,2)` [`r round(h1.b$hypothesis$CI.Lower,2)`, `r round(h1.b$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1.b$hypothesis$Post.Prob,3)`). Specifically, the model predicts a mean difference of `r round(mean(df.ms$dyad.diff),3)` [`r round(ci(df.ms$dyad.diff)$CI_low, 3)`, `r round(ci(df.ms$dyad.diff)$CI_high, 3)`] between non-autistic and mixed dyads as well as a mean differences of `r round(mean(df.ms$sync.diff),3)` [`r round(ci(df.ms$sync.diff)$CI_low, 3)`, `r round(ci(df.ms$sync.diff)$CI_high, 3)`] between high and low IPS. However, there is no support for our hypotheses regarding differences between the ratings of autistic and comparison observers (*estimate* = `r round(h1.c$hypothesis$Estimate,2)` [`r round(h1.c$hypothesis$CI.Lower,2)`, `r round(h1.c$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1.c$hypothesis$Post.Prob,3)`) or the interaction of IPS and dyad type (*estimate* = `r round(h1.d$hypothesis$Estimate,2)` [`r round(h1.d$hypothesis$CI.Lower,2)`, `r round(h1.d$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1.d$hypothesis$Post.Prob,3)`) or dyad type and diagnostic status of the observer (*estimate* = `r round(h1.e$hypothesis$Estimate,2)` [`r round(h1.e$hypothesis$CI.Lower,2)`, `r round(h1.e$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h1.e$hypothesis$Post.Prob,3)`). 


### Plots

Finally, we can plot our data!

```{r plot, fig.height=4}

# rain cloud plot for ratings
df.agg %>%
  group_by(subID, diagnosis, dyad.type, sync) %>%
  summarise(
    rating.confirmed = mean(rating.confirmed, na.rm = T)
  ) %>%
  rename(`IPS` = sync) %>%
  mutate(
    observer = recode(diagnosis,
                       "ASD" = "autistic observer", 
                       "COMP" = "comparison observer"),
    dyad.type = recode(dyad.type,
                       "mixed"        = "mixed dyad",
                       "non-autistic" = "non-autistic dyad")
  ) %>%
  ggplot(aes(dyad.type, rating.confirmed, fill = `IPS`, colour = `IPS`)) + 
  geom_rain(rain.side = 'r', 
            boxplot.args = list(colour = "black", 
                                outlier.shape = NA, 
                                show.legend = FALSE,
                                alpha = 0.75),
            violin.args  = list(colour = "black",
                                outlier.shape = NA, 
                                show.legend = T,
                                alpha = .75),
            point.args   = list(show.legend = FALSE,
                                size  = .75, 
                                alpha = .75),
            boxplot.args.pos = list(
              position = 
                ggpp::position_dodgenudge(x = .1, width = 0.1), 
              width = 0.1
            )) +
  ylim(0, 100) +
  facet_wrap(. ~ observer) +
  scale_fill_manual(values = c("#1E88E5", "#004D40")) +
  scale_color_manual(values = c("#1E88E5", "#004D40")) +
  labs(title = "Mean impression ratings", 
       x = "", 
       y = "rating") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))

ggsave("Figure3_ratings.pdf", 
       units = "mm", 
       width  = 170,
       height = 100,
       dpi    = 300)

```

## Fixation durations

We also collected eye tracking data during the presentation of the videos. From the eye tracking data, we extracted fixations using the algorithm from Nyström und Holmqvist (2010) and then calculated the duration of fixations on areas of interest (heads, hands and body). For testing the hypotheses, we calculated proportions of fixation durations from the total fixation duration for each video.  

### SBC

```{r sbc_setup_fix}

# set formula considering all combinations
code   = "PESI_fix"
f.fix = brms::bf(fix.prop ~ diagnosis * sync * dyad.type * AOI
                 + (sync * dyad.type * AOI | subID) 
                 + (diagnosis * sync * AOI | dyad))

# set weakly informed priors
priors = c(
  # three AOIs, therefore, intercept expected to be around 33%
  prior(normal(33, 11), class = Intercept), 
  prior(normal(11, 11), class = sigma), # 1/3 of the Intercept
  prior(normal(5,   5), class = sd),
  prior(lkj(2)        , class = cor),
  prior(normal(0,  10), class = b)
)

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # set the seed
  set.seed(2469)
  # create the data
  gen  = SBC_generator_brms(f.fix, data = df.fix.agg, prior = priors, 
                            thin = 50, warmup = 20000, refresh = 2000
  )
  dat  = generate_datasets(gen, nsim)
  saveRDS(dat, file = sprintf("%s/dat_%s.rds", cache_dir, code))
  
  # perform the SBC
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1, 
                                        warmup = warm, iter = iter,               
                                        inits = 0.1)
  res = compute_SBC(dat, bck,
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  # save the results dataframes
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  
}

```


Again, we start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). Although not perfect, we are satisfied with these numbers and continue. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_checks1_fix, fig.height=4}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.fix)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# plot simulated data for prior predictive checks
#dvmax = 100
dvfakematH = dvfakemat; 
#dvfakematH[dvfakematH > dvmax] = dvmax 
#dvfakematH[dvfakematH < 0] = 0 
breaks = seq(min(dvfakematH, na.rm = T)-1, max(dvfakematH, na.rm=T)+1, length.out = 100) 
binwidth = ceiling(breaks[2] - breaks[1])
breaks = seq(min(dvfakematH, na.rm = T)-1, max(dvfakematH)+binwidth, by = binwidth) 
histmat = matrix(NA, ncol = dim(dvfakematH)[2] + binwidth, nrow = length(breaks)-1) 
for (i in 1:dim(dvfakematH)[2]) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  #xlim(0, max(dvfakematH)) +
  theme_bw()

```

Again, most of our data fits our expectations, however, we chose wider priors, therefore, there are also simulated values outside of the possible values (0 to 100). 

```{r sbc_checks2_fix, fig.height=6}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Empirical cumulative distribution function")
plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Rank histograms")
plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Simulated values")
plot_contraction(
  df.results.b, 
  prior_sd = setNames(c(11, 
                        rep(10, length(unique(df.results.b$variable))-1)), 
                      unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3)) +
  ggtitle("Posterior contraction and z-scores")

```

Second, we check the ranks of the parameters. If the model is unbiased, these should be uniformly distributed (Schad, Betancourt and Vasishth, 2020). The sample empirical cumulative distribution function (ECDF) lies within the theoretical distribution (95%) and the rank histogram also shows ranks within the 95% expected range, although there are some small deviations. We judge this to be acceptable.

Third, we investigated the relationship between the simulated true parameters and the posterior estimates. Although there are individual values diverging from the expected pattern, most parameters were recovered successfully within an uncertainty interval of alpha = 0.05. 

Last, we explore the z-score and the posterior contraction of our population-level predictors. The z-score "determines the distance of the posterior mean from the true simulating parameter", while the posterior contraction "estimates how much prior uncertainty is reduced in the posterior estimation" (Schad, Betancourt and Vasisth, 2020). Both look acceptable. 

### Posterior predictive checks

As the next step, we fit the model and check for issues. 

```{r postpc_fix, fig.height = 24}

# fit the model
set.seed(2486)
m.fix = brm(f.fix,
            df.fix.agg, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_PESI_fix"
            )

# in this model, there are no divergent samples
sum(subset(nuts_params(m.fix), Parameter == "divergent__")$Value)

# check that rhats are below 1.01
sum(brms::rhat(m.fix) >= 1.01, na.rm = T)

# and the chains have converged
post.draws = as_draws_df(m.fix)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 2)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and only one rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_fix, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.fix, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.fix, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none") + xlim(0,100)

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.fix.agg$fix.prop, post.pred, df.fix.agg$diagnosis) + 
  theme_bw() + theme(legend.position = "none")
# ... sync level
p3 = ppc_stat_grouped(df.fix.agg$fix.prop, post.pred, df.fix.agg$sync) + 
  theme_bw() + theme(legend.position = "none")
# ... and dyad type
p4 = ppc_stat_grouped(df.fix.agg$fix.prop, post.pred, df.fix.agg$dyad.type) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3, p4,
          nrow = 2, ncol = 2, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks", 
                face = "bold", size = 14))

```

Although the overall shape is not perfect, the means are captured pretty well. Since we could not find a likelihood shape that fit better than this, we accept this model and test our hypotheses. 

### Model summary

Now that we are convinced that we can trust our model, we have a look at the model and its estimates.

```{r final_fix, fig.height=12}

# print a summary
summary(m.fix)

# plot the posterior distributions
post.draws %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), 
               names_to = "coef", 
               values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ASD"),
    coef = str_replace_all(coef, "sync1", "high sync"),
    coef = str_replace_all(coef, "dyad.type1", "mixed"),
    coef = str_replace_all(coef, "AOI1", "head"),
    coef = str_replace_all(coef, "AOI2", "body"),
    coef_order = case_when(
      coef == "ASD" ~ 100,
      coef == "high sync" ~ 99,
      coef == "mixed" ~ 98,
      coef == "head" ~ 97,
      coef == "body" ~ 96,
      T ~ 100-nchar(coef)),
    coef = fct_reorder(coef, coef_order)
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>% 
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

```


### Inferences

```{r inf_fix}

# H2.a Dyad type x AOI: Fixation durations for each area of interest differ
# between clips of no-diagnosis and mixed-diagnosis dyads.
h2.a = hypothesis(m.fix, "(dyad.type1:AOI1 + dyad.type1:AOI2)/2 < 0", alpha = 0.025)
h2.a

# H2.b Synchrony x AOI: Fixation durations for each AOI differ between 
# high- and low-synchrony clips. 
h2.b = hypothesis(m.fix, "(sync1:AOI1 + sync1:AOI2)/2 > 0", alpha = 0.025)
h2.b 

# H2.c Diagnostic status x AOI: Fixation durations for each AOI differ between 
# autistic and comparison observers
h2.c = hypothesis(m.fix, "(diagnosis1:AOI1 + diagnosis1:AOI2)/2 < 0", alpha = 0.025)
h2.c

## exploring which AOI is driving this
# first, reminder of the contrast settings
contrasts(m.fix[["data"]][["AOI"]])
contrasts(m.fix[["data"]][["diagnosis"]])

# ASD(body) > COMP(body)
e1.body = hypothesis(m.fix, "diagnosis1 - diagnosis1:AOI1 - diagnosis1:AOI2 > 0", alpha = 0.025)
e1.body
# ASD(hand) < COMP(hand)
e2.hand = hypothesis(m.fix, "-diagnosis1 - diagnosis1:AOI2 > 0", alpha = 0.025)
e2.hand
# ASD(head) < COMP(head)
e3.head = hypothesis(m.fix, "-diagnosis1 -diagnosis1:AOI1 > 0", alpha = 0.025)
e3.head

# extract predicted differences based on the model
df.new = df.fix %>% 
  select(diagnosis, dyad.type, sync, AOI) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, dyad.type, sync, AOI, sep = "_")
  )
df.ms = as.data.frame(
  fitted(m.fix, summary = F, 
               newdata = df.new %>% select(diagnosis, dyad.type, sync, AOI), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

# compute specific differences
df.ms = df.ms %>%
  mutate(
    ASD.body  = rowMeans(select(., matches("ASD.*_body")), na.rm = T),
    ASD.hand  = rowMeans(select(., matches("ASD.*_hand")), na.rm = T),
    ASD.head  = rowMeans(select(., matches("ASD.*_head")), na.rm = T),
    COMP.body = rowMeans(select(., matches("COMP.*_body")), na.rm = T),
    COMP.hand = rowMeans(select(., matches("COMP.*_hand")), na.rm = T),
    COMP.head = rowMeans(select(., matches("COMP.*_head")), na.rm = T),
    body.diff = COMP.body - ASD.body,
    hand.diff = COMP.hand - ASD.hand,
    head.diff = COMP.head - ASD.head,
    .keep = "none"
  )

# table for the cell values
st(df.ms, out = "kable")

```

This model revealed support for our expectation of differences between autistic and comparison observers dependent on the specific AOI (*estimate* = `r round(h2.c$hypothesis$Estimate,2)` [`r round(h2.c$hypothesis$CI.Lower,2)`, `r round(h2.c$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2.c$hypothesis$Post.Prob,3)`). Further exploration shows that this is driven by decreased dwell times on the head region for autistic observers (*estimate* = `r round(e3.head$hypothesis$Estimate,2)` [`r round(e3.head$hypothesis$CI.Lower,2)`, `r round(e3.head$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(e3.head$hypothesis$Post.Prob,3)`). Specifically, this model predicts a difference of dwell times on the heads of the interaction partners of `r round(mean(df.ms$head.diff), 3)`% [`r round(ci(df.ms$head.diff)$CI_low, 3)`, `r round(ci(df.ms$head.diff)$CI_high, 3)`] between autistic and comparison observers. There is no support for our hypotheses regarding interactions between IPS and AOI (*estimate* = `r round(h2.b$hypothesis$Estimate,2)` [`r round(h2.b$hypothesis$CI.Lower,2)`, `r round(h2.b$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2.b$hypothesis$Post.Prob,3)`) or dyad type and AOI (*estimate* = `r round(h2.a$hypothesis$Estimate,2)` [`r round(h2.a$hypothesis$CI.Lower,2)`, `r round(h2.a$hypothesis$CI.Upper,2)`], *posterior probability* = `r round(h2.a$hypothesis$Post.Prob,3)`). 

### Plots

At the end, we again plot our data. 

```{r plot_fix, fig.height=8}

# rain cloud plot for fixation durations
df.fix.agg %>%
  group_by(subID, diagnosis, sync, dyad.type, AOI) %>%
  summarise(
    fix.prop = mean(fix.prop, na.rm = T)
  ) %>%
  mutate(
    observer = recode(diagnosis,
                       "ASD" = "autistic", 
                       "COMP" = "comparison"),
    `IPS` = if_else(
      sync == "high", "high IPS","low IPS"
    ),
    dyad.type = recode(dyad.type,
                       "mixed"        = "mixed dyad",
                       "non-autistic" = "non-autistic dyad")
  ) %>%
  ggplot(aes(AOI, fix.prop, fill = observer, colour = observer)) + 
  geom_rain(rain.side = 'r', 
            boxplot.args = list(colour = "black", 
                                outlier.shape = NA, 
                                show.legend = FALSE,
                                alpha = .75),
            violin.args  = list(colour = "black",
                                outlier.shape = NA, 
                                show.legend = T,
                                alpha = .75),
            point.args   = list(show.legend = FALSE,
                                size  = 0.75,
                                alpha = .75),
            boxplot.args.pos = list(
              position = 
                ggpp::position_dodgenudge(x = .1, width = 0.1), 
              width = 0.1
            )) +
  ylim(0, 100) +
  facet_wrap(dyad.type ~ `IPS`) +
  scale_fill_manual(values = c("#CC79A7", "#D55E00")) +
  scale_color_manual(values = c("#CC79A7", "#D55E00")) +
  labs(title = "Mean dwell times", 
       x = "", 
       y = "percent") +
  theme_bw() + 
  theme(legend.position = "bottom", 
        plot.title = element_text(hjust = 0.5), 
        legend.direction = "horizontal", 
        text = element_text(size = 15))


ggsave("Figure4_dwelltimes.pdf", 
       units = "mm", 
       width  = 170,
       height = 170,
       dpi    = 300)

```

# Explorative analyses

We complement our hypotheses-guided analyses with some exploration of the data. 

## Number of saccades

First, we use the saccades extracted by the Nyström and Holmqvist (2010) algorithm to explore whether there are differences in saccadic behaviour between our groups or the conditions. 

### SBC

```{r sbc_setup_sac}

# set formula considering all combinations
code   = "PESI_sac_int"
f.sac = brms::bf(n.sac ~ diagnosis * sync * dyad.type
                 + (1 | subID) 
                 + (1 | dyad))

# set weakly informed priors
priors = c(
  # one saccade per second 
  prior(normal(2.3, 1.00), class = Intercept), 
  prior(normal(0,   0.20), class = sd),
  prior(normal(0,   0.10), class = b)
)

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # set the seed
  set.seed(2489)
  # create the data
  gen  = SBC_generator_brms(f.sac, data = df.sac.agg, prior = priors, 
                            thin = 50, warmup = 20000, refresh = 2000,
                            family = poisson
  )
  if (!file.exists(sprintf("%s/dat_%s.rds", cache_dir, code))) {
    dat  = generate_datasets(gen, nsim)
    saveRDS(dat, file = sprintf("%s/dat_%s.rds", cache_dir, code))
  } else {
    dat = readRDS(sprintf("%s/dat_%s.rds", cache_dir, code))
  }
  
  # perform the SBC
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1, 
                                        warmup = warm, iter = iter,               
                                        inits = 0.1)
  res = compute_SBC(dat, bck,
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  # save the results dataframes
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  
}

```

Again, we start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). We will have to look out for divergence issues in the final model. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_checks1_sac, fig.height=4}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.sac)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# set very large data points to a value of 200
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH > 100] = 100
# compute one histogram per simulated data-set 
breaks = seq(0, max(dvfakematH, na.rm=T), length.out = 100) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "number of saccades") +
  theme_bw()
p1

```

Our simulated data centres around observers only producing a couple of saccades per video, which is reasonable since the videos are only 10 seconds long. However, there is some data simulated that shows up to 50 saccades, so it is reasonably wide and we are happy with the prior predictive distribution. 

```{r sbc_checks2_sac, fig.height=15}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(
  df.results.b, 
  prior_sd = setNames(c(1, 
                        rep(0.1, length(unique(df.results.b$variable))-1)), 
                      unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

Second, we check the outcome of the SBC. All looks acceptable. 

### Posterior predictive checks

As the next step, we fit the model and check for issues. 

```{r postpc_sac, fig.height=9}

# fit the model
set.seed(2496)
m.sac = brm(f.sac,
            df.sac.agg, prior = priors,
            family = poisson,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_PESI_sac",
            save_pars = save_pars(all = TRUE)
            )

# in this model, there are no divergent samples
sum(subset(nuts_params(m.sac), Parameter == "divergent__")$Value)

# check that rhats are below 1.01
sum(brms::rhat(m.sac) >= 1.01, na.rm = T)

# and the chains have converged
post.draws = as_draws_df(m.sac)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 2)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and no rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_sac, fig.height=6}

# get the posterior predictions
post.pred = posterior_predict(m.sac, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.sac, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

# distributions of means and sds compared to the real values per group
p2 = ppc_stat_grouped(df.sac.agg$n.sac, post.pred, df.sac.agg$diagnosis) + 
  theme_bw() + theme(legend.position = "none")
# ... sync level
p3 = ppc_stat_grouped(df.sac.agg$n.sac, post.pred, df.sac.agg$sync) + 
  theme_bw() + theme(legend.position = "none")
# ... and dyad type
p4 = ppc_stat_grouped(df.sac.agg$n.sac, post.pred, df.sac.agg$dyad.type) + 
  theme_bw() + theme(legend.position = "none")

p = ggarrange(p1, p2, p3, p4,
          nrow = 2, ncol = 2, labels = "AUTO")
annotate_figure(p, 
                top = text_grob("Posterior predictive checks", 
                face = "bold", size = 14))

```

Although the overall shape is not perfect, the means are captured pretty well. Since we could not find a likelihood shape that fit better than this, we accept this model and test our hypotheses. 

### Model summary

Now, we have a look at the model and its estimates.

```{r final_sac, fig.height=6}

# print a summary
summary(m.sac)

# plot the posterior distributions
post.draws %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), 
               names_to = "coef", 
               values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef)),
    coef = str_replace_all(coef, ":", " x "),
    coef = str_replace_all(coef, "diagnosis1", "ASD"),
    coef = str_replace_all(coef, "sync1", "high sync"),
    coef = str_replace_all(coef, "dyad.type1", "mixed"),
    coef_order = case_when(
      coef == "ASD" ~ 100,
      coef == "high sync" ~ 99,
      coef == "mixed" ~ 98,
      T ~ 100-nchar(coef)),
    coef = fct_reorder(coef, coef_order)
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>% 
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

df.sac.overall = df.sac.agg %>% group_by(subID, diagnosis) %>% 
  summarise(n.sac = mean(n.sac, na.rm = T)) %>% 
  ungroup()

# explore difference between observer groups
e.sac = hypothesis(m.sac, "0 > diagnosis1", alpha = 0.025)

# extract predicted differences
df.new = df.sac.agg %>% 
  select(diagnosis, sync, dyad.type) %>% 
  distinct() %>%
  mutate(
    condition = paste(diagnosis, sync, dyad.type, sep = "_")
  )

df.ms = as.data.frame(
  fitted(m.sac, summary = F, 
               newdata = df.new %>% select(diagnosis, sync, dyad.type), 
               re_formula = NA))
colnames(df.ms) = df.new$condition

# calculate our difference columns
df.ms = df.ms %>%
     mutate(
       ASD  = rowMeans(select(., starts_with("ASD")), na.rm = T),
       COMP = rowMeans(select(., starts_with("COM")), na.rm = T),
       COMP_ASD = COMP - ASD
       )

```

In general, observers produced on average `r round(mean(df.sac.overall$n.sac),2)` +- `r round(sd(df.sac.overall$n.sac)/sqrt(nrow(df.sac.overall)),2)` saccades per 10-second segment. There were no credible differences between any of the task conditions or interactions. However, autistic observers produced fewer saccades than non-autistic observers (*estimate* = `r round(e.sac$hypothesis$Estimate,2)` [`r round(e.sac$hypothesis$CI.Lower, 2)`, `r round(e.sac$hypothesis$CI.Upper, 2)`], *posterior probability* = `r round(e.sac$hypothesis$Post.Prob,3)`). Specifically, the model predicts that autistic observers produce on average `r round(mean(df.ms$ASD),3)` [`r round(ci(df.ms$ASD)$CI_low, 3)`, `r round(ci(df.ms$ASD)$CI_high, 3)`] saccades, while comparison observers are predicted to produce `r round(mean(df.ms$COMP),3)` [`r round(ci(df.ms$COMP)$CI_low, 3)`, `r round(ci(df.ms$COMP)$CI_high, 3)`] saccades on average. 

## Predicting ratings by gaze behaviour

We also want to see whether across groups gaze behaviour predicted ratings, i.e., focus on specific AOIs led to increased ratings. 

### SBC

```{r sbc_check_com}

# combine data frames
df.com = df.fix %>%
  select(subID, diagnosis, video, sync, dyad, dyad.type, AOI, fix.dur) %>%
  pivot_wider(names_from = AOI, values_from = fix.dur) %>%
  merge(., df.sac.agg) %>%
  merge(., df) %>%
  group_by(subID, diagnosis, sync, dyad, dyad.type) %>%
  summarise(
    head  = mean(head), 
    hand  = mean(hand), 
    body  = mean(body), 
    n.sac = mean(n.sac),
    rating.confirmed = mean(rating.confirmed, na.rm = T)
  ) %>% ungroup() %>%
  # scale the predictors
  mutate(
    sn.sac = scale(n.sac),
    s.head = scale(head),
    s.hand = scale(hand), 
    s.body = scale(body)
  ) %>% drop_na()

# set formula
code   = "PESI_com"
f.com = brms::bf(rating.confirmed ~ sn.sac + s.head + s.hand + s.body + 
                   (1 | subID) + (1 | dyad))

# set weakly informed priors
priors = c(
  prior(normal(50,  10), class = Intercept),
  prior(normal(10,  10), class = sigma),
  prior(normal(0,   10), class = sd),
  prior(normal(0,   5), class = b)
)

if (file.exists(file.path(cache_dir, paste0("df_res_", code, ".rds")))) {
  # load in the results of the SBC
  df.results = readRDS(file.path(cache_dir, paste0("df_res_", code, ".rds")))
  df.backend = readRDS(file.path(cache_dir, paste0("df_div_", code, ".rds")))
  dat        = readRDS(file.path(cache_dir, paste0("dat_", code, ".rds")))
} else {
  # set the seed
  set.seed(2474)
  # create the data
  gen  = SBC_generator_brms(f.com, data = df.com, prior = priors, 
                            thin = 50, warmup = 20000, refresh = 2000
  )
  dat  = generate_datasets(gen, nsim)
  saveRDS(dat, file = sprintf("%s/dat_%s.rds", cache_dir, code))
  
  # perform the SBC
  bck = SBC_backend_brms_from_generator(gen, chains = 4, thin = 1, 
                                        warmup = warm, iter = iter,               
                                        inits = 0.1)
  res = compute_SBC(dat, bck,
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, sprintf("res_%s", code)))
  # save the results dataframes
  df.results = res$stats
  df.backend = res$backend_diagnostics
  saveRDS(df.results, file = file.path(cache_dir, paste0("df_res_", code, ".rds")))
  saveRDS(df.backend, file = file.path(cache_dir, paste0("df_div_", code, ".rds")))
  
}

```

Again, we start by investigating the rhats and the number of divergent samples. This shows that `r nrow(df.results %>% group_by(sim_id) %>% summarise(rhat = max(rhat, na.rm = T)) %>% filter(rhat >= 1.05))` of `r nsim` simulations had at least one parameter that had an rhat of at least 1.05, and `r nrow(df.backend %>% filter(n_divergent > 0))` models had divergent samples (mean number of samples of the simulations with divergent samples: `r as.numeric(df.backend %>% filter(n_divergent > 0) %>% summarise(n_divergent = round(mean(n_divergent), digits = 2)))`). Again, we will look out for divergence issues in the final model. 

Next, we can plot the simulated values to perform prior predictive checks. 

```{r sbc_checks1_com, fig.height=4}

# create a matrix out of generated data
dvname = gsub(" ", "", gsub("[\\|~].*", "", f.com)[1])
dvfakemat = matrix(NA, nrow(dat[['generated']][[1]]), length(dat[['generated']])) 
for (i in 1:length(dat[['generated']])) {
  dvfakemat[,i] = dat[['generated']][[i]][[dvname]]
}
truePars = dat$variables

# set very large data points to a value of 100 and small to 0
dvfakematH = dvfakemat; 
dvfakematH[dvfakematH > 100] = 100
dvfakematH[dvfakematH < 0]   = 0
# compute one histogram per simulated data-set 
breaks = seq(0, 100, length.out = 101) 
binwidth = breaks[2] - breaks[1]
histmat = matrix(NA, ncol = nrow(truePars) + binwidth, nrow = length(breaks)-1) 
for (i in 1:nrow(truePars)) {
  histmat[,i] = hist(dvfakematH[,i], breaks = breaks, plot = F)$counts 
}
# for each bin, compute quantiles across histograms 
probs = seq(0.1, 0.9, 0.1) 
quantmat= as.data.frame(matrix(NA, nrow=dim(histmat)[1], ncol = length(probs)))
names(quantmat) = paste0("p", probs)
for (i in 1:dim(histmat)[1]) {
  quantmat[i,] = quantile(histmat[i,], p = probs, na.rm = T)
}
quantmat$x = breaks[2:length(breaks)] - binwidth/2 # add bin mean 
p1 = ggplot(data = quantmat, aes(x = x)) + 
  geom_ribbon(aes(ymax = p0.9, ymin = p0.1), fill = c_light) + 
  geom_ribbon(aes(ymax = p0.8, ymin = p0.2), fill = c_light_highlight) + 
  geom_ribbon(aes(ymax = p0.7, ymin = p0.3), fill = c_mid) + 
  geom_ribbon(aes(ymax = p0.6, ymin = p0.4), fill = c_mid_highlight) + 
  geom_line(aes(y = p0.5), colour = c_dark, linewidth = 1) + 
  labs(title = "Prior predictive distribution", y = "", x = "number of saccades") +
  theme_bw()
p1

```

The overall shape again looks acceptable, even though there are some impossible simulated values. 

```{r sbc_checks2_com, fig.height=15}

# get simulation numbers with issues
rank = max(df.results$max_rank)
check = merge(df.results %>% 
    group_by(sim_id) %>% 
      summarise(
        rhat = max(rhat, na.rm = T), 
        mean_rank = mean(max_rank)
        ) %>% 
    filter(rhat >= 1.05 | mean_rank != rank), 
  df.backend %>% filter(n_divergent > 0), all = T)

# plot SBC with functions from the SBC package focusing on population-level parameters
df.results.b = df.results %>% 
  filter(substr(variable, 1, 2) == "b_") %>% 
  filter(!(sim_id %in% check$sim_id))
p1 = plot_ecdf_diff(df.results.b) + theme_bw() + theme(legend.position = "none") +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p2 = plot_rank_hist(df.results.b, bins = 20) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p3 = plot_sim_estimated(df.results.b, alpha = 0.5) + theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))
p4 = plot_contraction(
  df.results.b, 
  prior_sd = setNames(c(10, 
                        rep(5, length(unique(df.results.b$variable))-1)), 
                      unique(df.results.b$variable))) +
  theme_bw() +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

p = ggarrange(p1, p2, p3, p4, labels = "AUTO", ncol = 1, nrow = 4)
annotate_figure(p, 
                top = text_grob("Computational faithfulness and model sensitivity", 
                face = "bold", size = 14))

```

Second, we check the outcome of the SBC. All looks acceptable. 

### Posterior predictive checks

As the next step, we fit the model and check for issues. 

```{r postpc_com}

# fit the model
set.seed(2496)
m.com = brm(f.com,
            df.com, prior = priors,
            iter = iter, warmup = warm,
            backend = "cmdstanr", threads = threading(8),
            file = "m_PESI_com",
            save_pars = save_pars(all = TRUE)
            )


# in this model, there are no divergent samples
sum(subset(nuts_params(m.com), Parameter == "divergent__")$Value)

# check that rhats are below 1.01
sum(brms::rhat(m.com) >= 1.01, na.rm = T)

# and the chains have converged
post.draws = as_draws_df(m.com)
mcmc_trace(post.draws, regex_pars = "^b_",
           facet_args = list(ncol = 2)) +
  scale_x_continuous(breaks=scales::pretty_breaks(n = 3)) +
  scale_y_continuous(breaks=scales::pretty_breaks(n = 3))

```

This model has no divergent samples, and no rhat that is higher or equal to 1.01. Therefore, we go ahead and perform our posterior predictive checks. 

```{r postpc2_com, fig.height=4}

# get the posterior predictions
post.pred = posterior_predict(m.com, ndraws = nsim)

# check the fit of the predicted data compared to the real data
p1 = pp_check(m.com, ndraws = nsim) + 
  theme_bw() + theme(legend.position = "none")

annotate_figure(p1, 
                top = text_grob("Posterior predictive checks", 
                face = "bold", size = 14))

```

The predicted values in light blue fit the actual values in dark blue quite well. 

### Model summary

Now, we have a look at the model and its estimates.

```{r final_com, fig.height=6}

# print a summary
summary(m.com)

# plot the posterior distributions
post.draws %>% 
  select(starts_with("b_")) %>%
  pivot_longer(cols = starts_with("b_"), 
               names_to = "coef", 
               values_to = "estimate") %>%
  subset(!startsWith(coef, "b_Int")) %>%
  mutate(
    coef = substr(coef, 3, nchar(coef))
  ) %>% 
  group_by(coef) %>%
  mutate(
    cred = case_when(
      (mean(estimate) < 0 & quantile(estimate, probs = 0.975) < 0) |
        (mean(estimate) > 0 & quantile(estimate, probs = 0.025) > 0) ~ "credible",
      T ~ "not credible"
    )
  ) %>% ungroup() %>% 
  ggplot(aes(x = estimate, y = coef, fill = cred)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  ggdist::stat_halfeye(alpha = 0.7) + ylab(NULL) + theme_bw() +
  scale_fill_manual(values = c(credible = c_dark, c_light)) + 
  theme(legend.position = "none")

```

Predicting comfortability ratings with gaze behaviour revealed that higher comfortability impressions were associated with reduced dwell times on the hands (Scaled dwell times on hands: *estimate* = `r round(fixef(m.com)[4,1],2)` [`r round(fixef(m.com)[4,3],2)`, `r round(fixef(m.com)[4,4],2)`]) but no other gaze behaviour.
